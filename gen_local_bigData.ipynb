{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0150602b-a0f8-4c48-9e71-a8cd1db8acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import LocalCluster, progress\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c99a2b2e-2ea3-43ed-93cf-b14ce170404c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet directory set to: bigData\n"
     ]
    }
   ],
   "source": [
    "# %% Set Variables\n",
    "n = 1_000_000_000  # Total number of rows of data to generate\n",
    "chunksize = 10_000_000  # Number of rows of data per file\n",
    "std = 10.0  # Assume normally distributed temperatures with a standard deviation of 10\n",
    "\n",
    "os.chdir(\"C:\\\\Temp\") # set a working directory on the local drive due to file size generated\n",
    "# print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Lookup table of stations and their mean temperatures\n",
    "# this is a small file to use to generate big data file\n",
    "lookup_df = \\\n",
    "pd.read_csv(\"C:\\\\Users\\\\VHAATGGoldsD15\\\\OneDrive - Department of Veterans Affairs\\\\Documents\\\\_Python\\\\1trc\\\\lookup.csv\")  \n",
    "\n",
    "PARQUET_DIR = \"bigData\"\n",
    "#if os.path.exists(PARQUET_DIR):\n",
    "#    shutil.rmtree(PARQUET_DIR)\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "print(f\"Parquet directory set to: {PARQUET_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5d6a8b-7485-413b-9b6e-f59ec1f18052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Function Definition to Generate Data\n",
    "def generate_chunk(partition_idx, chunksize, std, lookup_df):\n",
    "    \"\"\"Generate some sample data based on the lookup table.\"\"\"\n",
    "    \n",
    "    rng = np.random.default_rng(partition_idx)  # Deterministic data generation\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            # Choose a random station from the lookup table for each row in our output\n",
    "            \"station\": rng.integers(0, len(lookup_df) - 1, int(chunksize)),\n",
    "            # Generate a normal distibution around zero for each row in our output\n",
    "            # Because the std is the same for every station we can adjust the mean for each row afterwards\n",
    "            \"measure\": rng.normal(0, std, int(chunksize)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Offset each measurement by the station's mean value\n",
    "    df.measure += df.station.map(lookup_df.mean_temp)\n",
    "    # Round the temprature to one decimal place\n",
    "    df.measure = df.measure.round(decimals=1)\n",
    "    # Convert the station index to the station name\n",
    "    df.station = df.station.map(lookup_df.station)\n",
    "\n",
    "    # Save this chunk to the output file\n",
    "    filename = f\"measurements-{partition_idx}.parquet\"\n",
    "    local = os.path.join(PARQUET_DIR, filename)\n",
    "    df.to_parquet(local, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033d1b3-17d9-4cfb-90f4-e93395efa14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate partitioned dataset\n",
    "cluster = LocalCluster(\n",
    "    n_workers=int(0.9 * mp.cpu_count()), # 90% of CPU cores\n",
    "    processes=True, # Use processes instead of threads\n",
    "    threads_per_worker=1, # One thread per worker\n",
    "    memory_limit='2GB', # Memory limit per worker\n",
    "    #n_workers=4, threads_per_worker=2\n",
    ")\n",
    "client = cluster.get_client()  # set up local cluster\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80582365-ff44-4b8d-bcdf-f0bf51200cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e22e09b05d4b2ea3b5067c907d4a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate partitioned dataset\n",
    "\n",
    "results = client.map(\n",
    "    generate_chunk,\n",
    "    range(int(n / chunksize)),\n",
    "    chunksize=chunksize,\n",
    "    std=std,\n",
    "    lookup_df=lookup_df,\n",
    ")\n",
    "progress(results) # this computes the delayed results and monitors to completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da9dd4ea-a2bb-421e-979e-091891b3f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close local cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d09276-186e-47eb-879d-feffb6e39375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
